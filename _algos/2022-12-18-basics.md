---
permalink: /algos/basics
title: "The Starting Point"
layout: single
classes: wide
toc: true
toc_label: "The Starting Point"
toc_icon: "cog"
excerpt: "The meat and potatoes of algorithms."
header:
  image: /assets/img/algos/basics/roadmap.png
  teaser: /assets/img/algos/basics/header-image.png
---
# Prologue
Alright, let's get technical. As I mentioned, we're going to start off
with most of the fundamentals someone needs. This post is relatively lengthy
and has two major sections but I've decided to keep all of this in one post,
rather than two since they're related.


#  Data Types
Let's begin by talking about the kind of data that we expect our algorithms
to work on. We need to be able to represent (at the minimum) the following:

1. Truth Values
2. Integers
3. Fractions
4. Alphabetical Characters (and Beyond)
5. References to Data Objects

The idea is that, these data types are just _types of values_ that we wish to operate on. As
we will see, computers need to find a way to store these things, and so we need to represent
what we want in a way that the hardware allows for.

## Bits
First of all, most modern computer systems [^1] like to store information
using bits. For our current intents and purposes, you can just take it to
mean that anything that we wish to store, the computer has to, and will
do it in the form of storing a sequence of values in $$0$$'s and $$1$$'s.

In your RAM sticks, in your hard drives, even when the data is 
being operated on in the CPU, it's all really just bits 
whizzing about circuits.

More commonly, we associate call every $$8$$ bits, a single _byte_. In fact,
we usually index into RAM (random access memory) data by bytes.

{:refdef: style="text-align: center;"}
![Puzzle](/assets/img/algos/basics/ram-layout.png)
{: refdef}
A silly diagram showing what bytes are in RAM.
{: style="color:gray; font-size: 80%; text-align: center;"}

So for this half of the post, you should take it that when we want to represent certain values,
they're sitting either in RAM, or on a drive as bytes. Let's see how the most 
important data types we want are represented.

## Booleans/Truth Values
It's going to be quite common that we need to evaluate the "truth value"
of a given statement. Take for example the following that tests to see
if a given number `a` is larger than the square root of another `b`:

{% highlight python %}
def square_test(a, b):
    # assumes both a and b are positive
    truth_value = (a * a) > b
    return truth_value
{% endhighlight %}

Think for a second about what the computer should store in `truth_value`
if it only sees bits. In Python (or a similar programming language)
we expect the type that is returned to be a boolean type 
that takes on value `True` or `False`. Most often, in computers 
this is represented by `1` and `0` respectively.

![](/assets/img/algos/basics/bool-rep.png)
How are are representing booleans.
{: style="color:gray; font-size: 80%; text-align: center;"}

Often times, programming languages also like to treat these as `1` and 
`0` respectively. For example, in `C/C++` it is possible to treat Boolean
values as integers and therefore do arithmetic or comparisons with 
other integers on them. An important distinction has to be made that 
when a programming language does it, it's an interpretation that 
`True` _can be viewed_ as `1` (and `False` as `0`). Booleans are not
integers, after all. 
_They may be represented the same way by computers but semantically, there is a difference between a "truth value" and an "integer"_.
{: .notice} 

## Representing Numbers and other Numerical Values
You might be used to seeing integers in represented in base 10, like so:
* 123
* 00123
* -270

In general, a number in base 10 is represented in the following way:

![](/assets/img/algos/basics/decimal.png){: style="width:100%; height:auto;"}
Base 10 in general.
{: style="color:gray; font-size: 80%; text-align: center;"}

However, there's really nothing stopping us from using base 2 instead. In fact, that's how
your computer stores integers. 

Here are some examples:
* $$ 3 =  1 * \left(1\right) + 1 * \left(2\right)  $$
* $$ 5 =  1 * \left(1\right) + 0 * \left(2\right) + 1 * \left(4\right)$$
* $$ 13 = 1 * \left(1\right) + 0 * \left(2\right) + 1 * \left(4\right) + 1 * \left(8\right)$$

So in binary, we would represent them like so:

![example representations](/assets/img/algos/basics/conversion-example.png)
What you see as opposed to what your computer sees.
{: style="color:gray; font-size: 80%; text-align: center;"}

Now theoretically, we could keep this going on forever. 
A small problem is that the arithmetic logic units (ALU) of 
modern processors can't operate on arbitrarily many bits
and as a consequence the maximum possible value represented
is actually limited. Typically, they allow up to either 32 bits, or 64 bits.
So, the largest possible value we can represent up to is either 
$$2^{32} - 1$$ or $$2^{64} - 1$$. For convention, whenever concerned, 
we will from now on take the integer representation to be using $$64$$ bits.

**Unsigned Integers**: For integers that are non-negative, we can store them as _unsigned integers_ 
in the manner as described above.
{: .notice--success}

### Adding Unsigned Integers
Now that we've introduced unsigned integers, let's cover addition up front (because it actually 
influences the way we represent the other numerical values).

**Question:** How would you add two numbers in binary?
{: .notice--primary}

It's actually really similar to how you would do it in decimal. Here's a simple sketch with
only $$4$$ bits:

![addition](/assets/img/algos/basics/addition-algo.png)
Addition algorithm on $$4$$ bits.
{: style="color:gray; font-size: 80%; text-align: center;"}

The fact that we discard the carry at the end makes our addition algorithm behave as sort of a 
"wraparound". So really think of this addition algorithm on $$4$$ bits as computing
$$(a + b)\  mod\  2^{4}$$.


### Signed Integers
But what about negative values? How should we represent them? One way would be to use one of the bits
to mark whether it's negative.

![Signed Integer](/assets/img/algos/basics/ones-complement.png)
Signed Representation.
{: style="color:gray; font-size: 80%; text-align: center;"}

But then we would need to first check the sign bit to see what kind of integers are involved,
and more importantly, based on that we need to either invoke the addition algorithm
or a subtraction algorithm.

Instead, it would be ideal to reuse the addition algorithm especially if we can use a representation 
which doesn't cost anything (compared to checking a sign bit before invoking either algorithm).

Recall that the addition algorithm we introduced for $$4$$ bits basically adds up to $$2^4 - 1= 15$$ before
wrapping around again. For example, $$2$$ and $$15$$ would be added to $$1$$ (try it). So what we can
do, is "reinterpret" the number line from $$0$$ to $$15$$ to accomodate for negative numbers. 
You can think of $$15$$ as being "$$1$$ away from $$0$$" which is basically $$-1$$.
In general, with $$4$$ bits, we would like to think of each number $$-x$$ as $$16 - x$$. The slight caveat 
is that with $$4$$ bits we can only represent $$16$$ numbers, so what we do is spend 
$$8$$ of them representing $$0$$ to $$7$$, and the remaining numbers representing $$-8$$ to $$-1$$.

![Which numbers we choose to represent](/assets/img/algos/basics/number-line-with-chosen.png)
Which numbers we choose to represent.
{: style="color:gray; font-size: 80%; text-align: center;"}

So if you're given a number like $$5$$ ($$0101$$ in binary), what would be its negative 
equivalent? It would be $$16 - 5 = 11$$ ($$1011$$ in binary). The nice thing now is that we 
can add any two numbers between $$-8$$ and $$7$$ and it just works, as long as we interpret 
our negative numbers in the way that I've mentioned.

Now a "quick" way for computers to convert numbers to their negatives, is to do the following:

![How processors actually negate numbers](/assets/img/algos/basics/fast-conversion.png)
How processors actually negate numbers.
{: style="color:gray; font-size: 80%; text-align: center;"}

This might sound like magic, but that's really happening is that we're really just computing
$$16 - x$$. Think of it this way: $$1111_2$$ is $$15 = 2^4 - 1$$. So subtracting a number from $$15$$
, like $$4$$, just removes the bits from $$15$$ that $$4$$ has. In which case, we would get $$1011_2$$.
This holds regardless of what number we're subtracting from $$15$$, as long as it's less
than or equal to $$15$$ (try it!).

"Removing bits from $$1111_2$$" is really just the same as flipping all of the bits. Now, we really wanted $$16 - x$$, not $$16 - 1 - x$$. So, what we need to do, is add $$1$$ after flipping the bits.

Et voila! Welcome to what is known as [two's complement](https://en.wikipedia.org/wiki/Two%27s_complement).

**Signed integers:** For integers that _might be_ negative, we can store them as _signed integers_ 
in the manner as described above. 
{: .notice--success}

In practice, you can make signed and unsigned integers for 32bits, 64 bits, and so on. We can only expect them to go up as large as $$2^{63} - 1$$ for unsigned, or 
$$2^{62} - 1$$ for signed integers.
{: .notice} 

**Warning:** Note that signed integers and unsigned integers are different types.
So adding unsigned integers to signed integers might produce unexpected results since the type tells us
how we should be interpreting the numbers.
{: .notice--warning} 

### What about fractional values? A Sketch on Floating Point Values
What happens if we want to represent numbers in between integers? What about $$\frac{1}{3}$$? What about 
$$0.55$$? Relating it back to the decimal system that we're all comfortable with, we can try a format 
that looks something like this:

![](/assets/img/algos/basics/mantissa-exponent.png)
An example of how we might begin to start storing fractional values.
{: style="color:gray; font-size: 80%; text-align: center;"}

We've not completely dodged the issue, we somehow still need to store a "fractional" value in the mantissa
although now the possible range is restricted from $$1$$ to $$10$$ (in some sense). Worse still, it's still
 unclear how we would store something like $$1/3$$, which actually can't be finitely written in a decimal 
point system.

The sad fact is that we won't be able to store values up to an arbitrary precision. As a very simple 
example, let's say you only had $$4$$ bits for your representation. This means you can only 
store up to $$2^4 = 16$$ possible values. How would you go about storing $$0.3$$, $$0.33$$, $$0.333$$,
and so on, up to $$0.3333333333333333$$ and $$0.33333333333333333$$? That's 17 possible values,
but with only $$4$$ bits, some of these need to have the same bit representation. So instead, 
the representation that we're going for, with limited bits means limited precision. Let's explain what this 
means in the binary system.

![](/assets/img/algos/basics/example-binary-point.png)
Fractional binary values.
{: style="color:gray; font-size: 80%; text-align: center;"}

The above example is if we had $$6$$ bits, and notice that with $$3$$ bits for the fractional part,
we can store any fractional value in the range of $$0$$ to $$0.875$$. Normally, for a mantissa,
we would all $$6$$ bits to the right of the binary point. This means we store values between
$$0$$ and $$1$$.

Now the rough idea, on how to store fractional values in binary, is the following:
1. Use 1 bit for the sign.
2. Use some bits for the mantissa (which stores a value between $$0$$ and $$1$$).
3. Use some bits for the exponent (which is offset from a negative number).

Here's an example with $$8$$ bits.

![](/assets/img/algos/basics/floating-example.png)
Rough idea with $$8$$ bits.
{: style="color:gray; font-size: 80%; text-align: center;"}

Note that in the exponent, the value stored needs to interpreted as an offset added to $$-(2^2 - 1) = -3$$,
since we're using $$3$$ bits for the exponent. The reason why we do this is because we also want negative
powers, not just nonnegative ones.
In general, for $$b$$ bits, the exponent is added to $$-(2^b - 1)$$. At this point you might notice in the 
example I've shown you guys, there's more than one way to represent the same value by.

The actual way that single and double precision values are represented as specified in 
the [IEEE 754 standard](https://en.wikipedia.org/wiki/IEEE_754) actually has a few optimisations
and deals with edge cases that we won't cover here. Just a simple understanding of how it roughly works 
is good enough for our purposes.

For _single-precision_ values ($$32$$-bit representation), $$23$$ bits are used for the mantissa, and 
$$8$$ bits are used for the exponent. For _double-precision_ values ($$64$$-bit representation),
$$52$$ bits are used for the mantissa, and $$11$$ bits are used for the exponent.

**Floating Point Values:** Single-precision binary floating-point values are stored in the _binary32_ 
format using 32 bits. 
Double-precision binary floating-point values are stored in the _binary64_ format using 64 bits. 
{: .notice--success}


## Characters
Numerical values aren't all there is to represent for computers. After all, besides performing mathematical
calculations, we might want to manipulate things like databases of names, urls and so on.

The way that this is done is via the character data type. The standard way that characters are encoded is 
based on [American Standard Code for Information Interchange (**ASCII**)](https://en.wikipedia.org/wiki/ASCII#Overview).

Here's a table that maps characters to their values that are stored in computer memory:

{:refdef: style="text-align: center;"}
![](https://www.asciitable.com/asciifull.gif)
{: refdef}
ASCII table from [www.asciitable.com](https://www.asciitable.com).
{: style="color:gray; font-size: 80%; text-align: center;"}

Some notable aspects include that there are non-printable characters in there, like `DEL` (delete) or 
`NUL` (null terminator). Also, besides letters, there are other things that are considered characters,
like `+` or `"`.

The above table only shows the first $$2^7 = 128$$ characters. As a standard, character data types are
1 byte large (or 8 bits), encoding up to 256 characters (the remaining 128 characters are in the 
extended ASCII table).

![](/assets/img/algos/basics/ascii-conversion.png)
How characters are stored.
{: style="color:gray; font-size: 80%; text-align: center;"}


Nowadays, a popular encoding is the 
[Unicode Transcode Format -- 8-bit(UTF-8)](https://en.wikipedia.org/wiki/UTF-8), which is instead a variable 
length encoding. It encodes [unicode characters](https://home.unicode.org/), which includes the original
set of ASCII characters, and beyond.

**Characters:** We will can store anything that is ascii encodable as a _character_. 
{: .notice--success}


### A Small Puzzle

Here's a puzzle you can play around with:
{:refdef: style="text-align: center;"}
![Puzzle](/assets/img/algos/basics/puzzle.png)
{: refdef}
A receipt with some printing errors. Source of image included in [solution section](/algos/basics#epilogue-puzzle-answers).
{: style="color:gray; font-size: 80%; text-align: center;"}

The original intended text is in English and the hint is that the encoding system used is ASCII. 
Can you propose a theory for why the receipt has turned out like this? A solution is included 
at the [end of this post](/algos/basics#epilogue-puzzle-answers).

## Arrays, Strings, and Big Integers
It would be handy to store sequences of characters or sequences of other things.
An array type can be thought of as a collection of other types. So for example, an array 
of integers can just be thought of as a list of integers, all stored adjacently. Now this lines 
up with how hardware stores data.

For our convention of arrays, you should think of them as something that is _indexable_. That is
to say, given an array `arr` of size $$n$$, then `arr[0]` holds the first element,
`arr[1]` holds the second element, and so on until `arr[n - 1]` that holds the last element.

For the sake of simplicity, let's say that we wanted to store 4 integers that are 2 bytes each.
This amounts to 8 bytes.

{:refdef: style="text-align: center;"}
![arrays](/assets/img/algos/basics/array-layout.png)
{: refdef}
How arrays are seen vs what your computer sees.
{: style="color:gray; font-size: 80%; text-align: center;"}

**Arrays:** We will store a sequence of type `T` as an _array of `T`_. 
{: .notice--success}

### Arrays for common types
So essentially, we can think of strings (sequence of characters) as really an array of characters. The 
string "Words" can really be thought of as a sequence ['W', 'o', 'r', 'd', 's'].

Also, recall that the integers we have talked about until now have a limited range of values that they can 
represent. One thing is that we can represent larger numbers using arrays of unsigned integers instead.
Such numbers would have a different base, and each element of the array would represent a digit.

For example, 560 in decimal can be treated as a size 3 array like [5, 6, 0]. There's nothing restricting us
from changing the base of the representation to something else. So if we used an array of unsigned 32 bit
integers, we can think of each digit as taking a value in the range of $$0$$ to $$2^{32} - 1$$ (and 
thus also the representation as having that base).

In such a case, we would need to implement our own arithmetic algorithms (or rely on a library), 
since most modern computers don't support operations for such representations.

Someday I might make a post about big integer representations.

For now, just take it to mean that we can indeed support arbitrarily large numbers if we really wanted to.

## Basic Memory Layouts, and How Pointers Fit In

Now the last thing that we need to cover are pointers/references. In programming 
languages there is somewhat of a difference between what these two objects are, but for 
the purposes of my series of posts, it will not matter too much.

We first need to talk a little bit about memory layout. 
Not any of the complicated stuff, just the relevant parts.

{:refdef: style="text-align: center;"}
![arrays](/assets/img/algos/basics/stack-heap.png)
{: refdef}
Stacks and Heaps in Memory.
{: style="color:gray; font-size: 80%; text-align: center;"}

So often times in algorithms, it's ideal to call functions that help us handle subroutines.
Every time a function is called, a _stack frame_ is allocated on the _stack_ and the local variables
(variables local to the function) are stored on the stack frame. In the example above, 
variables `x`, `a`, `to_return`, `local_str`, and `str` are local variables so they are stored on their 
respective stack frames. You'll notice that on the stack, values like `25`, `"local"`, `5` are present.

But what is this `16`? One way to keep something persistent across function calls, 
is to store it on the _heap_ instead, which is a section that persists across function calls. In fact,
data here persists from the start of the program, until the end. For example, `"hello"` 
is a string that we would like to keep persistent across function calls, so we should store it on the heap.
But how do we let the functions access it? One way is to give it a handle, which is called a pointer
or a reference [^3]. When functions are given a pointer/reference, they first need to "dereference" it.
What this means is that they need to look at the value itself, in this case `16`, and go into the heap
and look up memory location `16` (for simplicity, think of this as the 16th byte in the heap) to start
getting the string. So a pointer/reference, is not the object itself, but rather an address to 
where the object is on heap. It literally refers to, or points at, the object we are storing.

**Pointers of type T:** If a pointer points to something on heap of type `T`, we 
call it a _`T` pointer_. 
{: .notice--success}

### So Why Pointers?
{: .no_toc}

This is actually a difficult question to answer right now, but I just wanted to make a section to 
assure you that most of the reasons will be apparent either later in this post, or a little later 
on when we talk about data structures.


## Composing Into More Complicated Types
Often times, we will find ourselves grouping types together because it will be handle to make our own
"types" which are really just composed of the basic types I've already mentioned. An example might be 
a "coordinate" type, which should really just be pair of double precision floating point values.

Another example might be a "person" type, which stores their name as a string, their name as an
unsigned integer, and maybe their height, as a double precision floating point value.

One common type that we will talk about later on, is a "linked list node" type, which among other things,
stores a pointer that points to a linked list node. We will see what this accomplishes later on.

{:refdef: style="text-align: center;"}
![arrays](/assets/img/algos/basics/composite-types.png)
{: refdef}
Examples of some composite types we can make.
{: style="color:gray; font-size: 80%; text-align: center;"}

For now, note that we cannot have a compose a type with itself. Because if we did that,
that would be a circular definition! One way around this, is for us to use pointers. I.e. A node won't 
hold a node, but rather a reference/pointer to a node.

# Basic Operations

## The Computational Model


## Copying vs. Moving

## Subroutines

## A Philosophical Aside

## The Efficiency of Algorithms

# Time and Space Complexity


## The Notion of Asymptotics

### Bachmannâ€“Landau Notation 

### The Limits of Asymptotics: Where the real world departs from the model

# What Next?

# Epilogue: Puzzle Answers
Notice that some words remain unaffected, like the word "Change" [^2]. Let's look at some of 
the ascii values for the characters in that word.


| Character   | ASCII Value | Binary Representation of ASCII Value|
| ----------- | ----------- | ----------- |
| C           |    67       |  01000011   |
| a           | 97          |  01100001   |
| n           | 110         |  01101110   |

See how the 5th least significant bit is always 0, and furthermore for any of the other bits, they
they seem to be unaffected. (We can't test the most and second most sigificant bit using this word).

The leading theory could be that the 5th least significant bit is stuck on 0, whereas all of the other
bits are kept faithfully. Let's test this out.

Let's use a word like "McDoeble". You can check that the ascii value every character in that word
does not have the 5th bit in its binary representation. The original word should be "McDouble", 
and lo and behold, the ASCII Value of 'u' is 117, or 01110101 in binary. If we mask out
the 5th bit, we get 01100101 in binary, 101 in decimal, or the letter 'e'!

Equivalently, "Fbench Fbiec" should be "French Fries" and if you check the relationship between
'b' and 'r' or the relationship between 'c' and 's', you'll notice that indeed the 5th bit is always set
to 0.

So in all likelihood, the 5th bit is stuck somehow. How might that be? Receipt printers are usually
connected to cash registers via ribbon cables, like the one below. 

{:refdef: style="text-align: center;"}
![Puzzle](/assets/img/algos/basics/ribbon-cable.jpeg)
{: refdef}
An image of two ribbon cables, taken from [Wikipedia](https://en.wikipedia.org/wiki/Ribbon_cable#/media/File:Flachbandkabel.jpg).
{: style="color:gray; font-size: 80%; text-align: center;"}

That's multiple parallel wires that supply information to the printer. One guess is that there were 
8 of such parallel wires that would send out 8 bits for the letter encoded in ascii, and the 5th wire
was frayed or the connection was loose, so the bit was always stuck on 0. 

Now this isn't an isolated incident. In computer RAM, sometimes bits can be stuck too, and when
your computer reads the bits from it to print characters, the same thing happens.

{:refdef: style="text-align: center;"}
![Stuck RAM Bit](/assets/img/algos/basics/ram-stuck-bit.jpeg)
{: refdef}
Stuck RAM Bit Causing Print Issues, taken from [Imgur](https://imgur.com/M3sni).
{: style="color:gray; font-size: 80%; text-align: center;"}

Hope this was fun!
---
[^1]: I say most, because things like ternary CMOS and quantum computers exist.
[^2]: Which is ironic because it remain unchanged during printing. Haw haw haw.
[^3]: It is common in programming languages that there are subtle
differences between what the terms "references" and "pointers" refer to, but let's not go into that.